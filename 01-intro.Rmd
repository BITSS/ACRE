---
output:
  html_document: default
  pdf_document: default
  word_document: default
---


# Introduction {#intro} 

In 2019, the [American Economic Association](https://www.aeaweb.org/journals/policies/data-code/) began requiring all the materials for reproduction of any new publication. In addition to the requirements, several [specific recommendations](https://aeadataeditor.github.io/aea-de-guidance/) were produced to facilitate the authors' compliance. This change in policy and guidance should radically improve the computational reproducibility of all new published research. However all the published research before 2019 did not have neither the requirements nor the guidance to develop proper reproduction package.

As a result, several studies point towards rates of computational reproducibility in economics that range from alarmingly low [@galiani2018make; @chang2015economics] to just low [@kingi2018reproducibility]. The project of Advancing Computational Reproducibility in Economics (ACRE) has the goals of improving, and having a much clearer picture of, computational reproducibility in economics. 

*Replication*, or the process by which a study’s hypotheses and findings are re-examined using different data or different methods (or both) [@King95] is an essential part of the scientific process that allows science to be “self-correcting.” *Computational reproducibility*, or the ability to reproduce the results, tables, and other figures using the available data, code, and materials, is a precondition for replication. Computational reproducibility is assessed through the process of *reproduction*. At the center of this process is the *reproducer* (you!), a party not involved in the production of the original paper. Reproductions sometimes involve the *original author* (whom we refer to as “the author”) in cases where additional guidance and materials are needed to execute the process.

This exercise is designed for reproductions performed in Economics graduate courses or undergraduate theses, with the goal of providing common terminology and standards for conducting reproductions. The goal of reproduction, in general, is to assess and improve the computational reproducibility of published research in a way that facilitates future replication, extension, and collaboration.

This exercise is part of the Advancing Computational Reproducibility in Economics (ACRE) project led by the Berkeley Initiative for Transparency in the Social Sciences [(BITSS)](bitss.org) and Prof. Lars Vilhuber, Data Editor for the journals of the American Economic Association (AEA). The ACRE project builds on efforts to improve the reproducibility of economics research through the updated [Data and Code Availability Policy](https://www.aeaweb.org/journals/policies/data-code/), which since 2019 mandates pre-publication verification of computational reproducibility by the AEA Data Editor. Several [specific recommendations](https://aeadataeditor.github.io/aea-de-guidance/) were also produced to facilitate the authors' compliance.

## Beyond binary judgments

Assessments of reproducibility can easily gravitate towards binary assessments that declare a entire paper "reproducible" or "non-reproducible". These guidelines suggest a more nuance approach by highlighting three dimensions that make binary judgement less relevant. First, one paper usually contains several scientific claims, and those claims can vary in computational reproducibility. Second, each claim is typically measured through several specifications. This makes easier for reproducers to hypothesize after the fact about which are the main specifications where reproducibility assessed. Finally, there are several levels of reproucibility, ranging from the absence of any materials to the complete reproducibility starting from the raw data. And even for a specific claim-specification, distinguishing the appropriate level can be far more constructive than simple labeling as (ir)reproducible.


```{r diagram, echo=FALSE, fig.cap="Components to reproduce in a paper"}
# JOEL: can you figure out why this to run int the html version of the book?
library(DiagrammeR)

grViz("
digraph a_nice_graph {

graph [layout = neato, rankdir= TB]  ## layout = [neato|twopi, etc]
#https://rich-iannone.github.io/DiagrammeR/graphviz_and_mermaid.html


# node definitions with substituted label text
node [fontname = Helvetica, shape = box, style=filled ]
paper [label = '@@1']    ## label indicates the position of the letter

node [fontname = Helvetica, shape = diamond, fontsize = 10, fixedsize = TRUE]
claim1 [label = '@@2-1']
claim2 [label = '@@2-2']
claim3 [label = '@@2-3']

node [fontname = Helvetica, shape = circle, fillcolor=YellowGreen, fixedsize = TRUE]
output1 [label = '@@3-1']
output2 [label = '@@3-2']
output3 [label = '@@3-3']
output4 [label = '@@3-4']
output5 [label = '@@3-5']
output6 [label = '@@3-6']

node [fontname = Helvetica, shape = circle, fixedsize = TRUE, fillcolor=Peru]
spec1 [label = '@@4-1']
spec2 [label = '@@4-2']
spec3 [label = '@@4-3']
spec4 [label = '@@4-4']
spec5 [label = '@@4-5']
spec6 [label = '@@4-6']
spec7 [label = '@@4-7']
spec8 [label = '@@4-8']
spec9 [label = '@@4-9']
spec10 [label = '@@4-10']
spec11 [label = '@@4-11']
spec12 [label = '@@4-12']


# edge definitions with the node IDs
paper -> {claim1 claim2 claim3}       ##[label = ...] adds text on the edge
claim1 -> {output1 output2} 
claim2 -> {output3 output4} 
claim3 -> {output5 output6} 
output1 -> {spec1 spec2} 
output2 -> {spec3 spec4} 
output3 -> {spec5 spec6} 
output4 -> {spec7 spec8} 
output5 -> {spec9 spec10} 
output6 -> {spec11 spec12} 
}

[1]: 'Paper'            ## adds label to each box
[2]: c('Claim 1', 'Claim 2', 'Claim 3')
[3]: c('Opt 1' , 'Opt 2', 'Opt 3', 'Opt 4', 'Opt 5', 'Opt 6')
[4]: c('S1', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'S10', 'S11', 'S12')
[5]: 'ghfd'
")



# DiagrammeR("
#    graph TB
#    Paper-->Claim1
#    Paper-->Claim2
#    Paper-->Claim3
#    Claim1-->Spec11
#    Claim1-->Spec12
#    Claim1-->Spec13
#    Claim2-->Spec21
#    Claim2-->Spec22
#    Claim2-->Spec23
#    Claim3-->Spec31
#    Claim3-->Spec32
#    Claim3-->Spec33
#    ")
```

It should be noticed that the highest level of reproducibility, which requires complete reproducibility starting from the raw data, is very demanding and it should not be expected of published research (especially that published before 2019). This high bar is set up as an aspiration to improve the current reproducibility of research and facilitate the transmission of knowledge in the scientific community. 


## Stages of the exercise

This reproduction exercise is divided into four stages, with a fifth optional stage :   

1.	**Scoping**, where the reproducer defines the scope of the exercise by declaring a paper and specific output(s) on which they will focus;  
2.	**Assessment**, where the reproducer reviews and describes in detail the available reproduction package (or the “reproduction materials”) and assess current levels of computational reproducibility;  
3.	**Improvement**, where the reproducer modifies the content and/or the organization of the reproduction package to improve its reproducibility;  
4.	**Robustness checks**, where the reproducer assesses the quality of selected analytical choices from the paper; and  
5.	**Extension** (if applicable), where the reproducer may “extend” the current paper by including new methodologies or data. This step brings the reproduction exercise a step closer to replication.


                    (1)       (2)         (3)        (4)        (5)
                  scope --> assess --> improve --> robust --> extend
                   ▲         |  |                   ▲
                   |         |  |                   |
                   |_________|  |___________________|

            Suggested level of effort:
        - Graduate
          research:   5%       10%        5%         10%         70%
        - Graduate
          course:    10%       25%       20%         40%         5%
        - Undergrad.
          thesis:    10%       30%       40%         20%         0%

(**JOEL**: Please add guesstimates of level of efforts for projects that last 2 weeks, 1 month or one semester. Ask Katie for a paper form 3ie on Push button replications (one of the authors is Sayak Khatua. From Katie: Working paper here says 3 hours per replication: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/FPNITS)

Figure 1 depicts suggested levels of effort for each stage of the exercise depending on the context in which you are performing a reproduction. This process need not be chronologically linear, e.g., you may realize that the scope of a reproduction is too ambitious and switch to a less intensive reproduction. Later in the exercise, you can also begin testing different specifications for robustness while also assessing a paper's level of reproducibility.

## Recording the results of the exercise

You will be asked to record the results of your reproduction progress through each stage. After [scoping](#scoping), you will be asked to complete [a survey](https://berkeley.qualtrics.com/jfe/form/SV_8hLHNI6LGSYchEN) recording your paper of choice and specific outputs you will attempt to reproduce. This step also involves writing a brief 1-2 page summary of your chosen paper. In [assessment](#assessment), you will inspect the paper's reproduction package (raw data, analysis data, and code), draw diagrams connecting the **output to be reproduce** with its inputs, and score the level of reproducibility. All this information will be recorded in a [standarized spreadsheet](ADD LINK). In [improvement]((#improvements)), a second short survey will ask you to record specific improvements and report potential increases in the level of reproducibility. Finally, in [robustness checks](#robust) you will record any analytical choices and their possible variations.
