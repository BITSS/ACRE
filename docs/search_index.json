[
["index.html", "Guidelines for Computational Reproducibility in Economics", " Guidelines for Computational Reproducibility in Economics ACRE Team 2020-03-24 (1) (2) (3) (4) (5) scope --&gt; assess --&gt; improve --&gt; robust --&gt; extend ▲ | | ▲ | | | | |_________| |___________________| (1) Scoping (2) Assessment (3) Improvement (4) Robustness Extensions Outcome-level Paper-level ☐ Select paper ☐ Describe inputs ☐ + Raw data ☐ + Version control ☐ Analytical choices ☐ New method ☐ Check ACRE ☐ Reproduction diagrams ☐ + Analysis data ☐ + Documentation ☐ Type of choice ☐ New data ☐ Check Rep. pkg exists ☐ Reproduction score ☐ + Analysis code ☐ + Dynamic document ☐ Choice value ☐ New data ☐ Read paper ☐ Debug analysis code ☐ + File structure ☐ Justify and test alternatives ☐ Declare estimates ☐ Debug cleaning code ☐ Debug cleaning code Record results is Survey 1 Record results is Survey 2 Record results is Survey 3 "],
["intro.html", "Introduction Beyond binary judgments Stages of the exercise Recording the results of the exercise", " Introduction In 2019, the American Economic Association updated its Data and Code Availability Policy, which now mandates that the AEA Data Editor verifies the reproducibility of all papers before they are accepted by an AEA journal. In addition to the requirements, several specific recommendations were produced to facilitate compliance. This change in policy is expected to improve the computational reproducibility of all newly published research, after several studies showed that rates of computational reproducibility in economics at large range from alarmingly low (Galiani, Gertler, and Romero 2018; Chang and Li 2015) to just low (Kingi et al. 2018). Replication, or the process by which a study’s hypotheses and findings are re-examined using different data or different methods (or both) (King 1995) is an essential part of the scientific process that allows science to be “self-correcting.” Computational reproducibility, or the ability to reproduce the results, tables, and other figures using the available data, code, and materials, is a precondition for replication. Computational reproducibility is assessed through the process of reproduction. At the center of this process is the reproducer (you!), a party not involved in the production of the original paper. Reproductions sometimes involve the original author (whom we refer to as “the author”) in cases where additional guidance and materials are needed to execute the process. This exercise is designed for reproductions performed in economics graduate courses or undergraduate theses, with the goal of providing a common approach, terminology, and standards for conducting reproductions. The goal of reproduction, in general, is to assess and improve the computational reproducibility of published research in a way that facilitates future replication, extension, and collaboration. This exercise is part of the Advancing Computational Reproducibility in Economics (ACRE) project led by the Berkeley Initiative for Transparency in the Social Sciences (BITSS) and Prof. Lars Vilhuber, Data Editor for the journals of the American Economic Association (AEA). The project of Advancing Computational Reproducibility in Economics (ACRE) looks to assess, enable, and improve the computational reproducibility of published economics research. Beyond binary judgments Assessments of reproducibility can easily gravitate towards binary assessments that declare an entire paper “reproducible” or “non-reproducible”. These guidelines suggest a more nuanced approach by highlighting two reasons that make binary judgment less relevant. First, a paper may contain several scientific claims, out of which all can vary in computational reproducibility. Each claim is tested using different methodologies where results are presented in one ore more outputs (like table and figures). Each output will itself contain several specifications. Figure 0.1 illustrates this idea. Figure 0.1: One paper has multiple components to reproduce Second, for a given specification there are several levels of reproducibility, ranging from the absence of any materials to complete reproducibility starting from the raw data. And even for a specific claim-specification, distinguishing the appropriate level can be far more constructive than simple labeling as (ir)reproducible. Note that the highest level of reproducibility, which requires complete reproducibility starting from raw data, is very demanding and it should not be expected of all published research (especially before 2019). Instead, this standard is set up as an aspiration to improve the current reproducibility of research and facilitate the transmission of knowledge in the scientific community. Stages of the exercise This reproduction exercise is divided into four stages, corresponding to the first four chapters of these guideleines, with an fifth optional stage : Scoping, where you (the reproducer) wil define the scope of the exercise by declaring a paper and the specific output(s) on which they will focus; Assessment, where you will review and describe in detail the available reproduction package (or the “reproduction materials”) and provides an assessment of the current level of computational reproducibility of the selected outputs; Improvement, where you will modify the content and/or the organization of the reproduction package to improve its reproducibility; Robustness checks, where will assess the quality of selected analytical choices from the paper; and Extension (if applicable), where you may extend the current paper by including new methodologies or data. This step brings the reproduction exercise a step closer to replication. Figure 2: Steps for reproduction (1) (2) (3) (4) (5) scope --&gt; assess --&gt; improve --&gt; robust --&gt; extend ▲ | | ▲ | | | | |_________| |___________________| Suggested level of effort: - Graduate research: 5% 10% 5% 10% 70% - Graduate course: 10% 25% 20% 40% 5% - Undergrad. thesis: 10% 30% 40% 20% 0% Figure 2 depicts suggested levels of effort for each stage of the exercise depending on the context in which the reproducer is performing a reproduction. This process need not be chronologically linear, e.g., the reproducer may realize that the scope of a reproduction is too ambitious and switch to a less intensive reproduction. Later in the exercise, the reproducer can also begin testing different specifications for robustness while also assessing a paper’s level of reproducibility. Recording the results of the exercise You will be asked to record the results of their reproduction progress through each stage. As part of Stage 1:Scoping, they will be asked to complete a first survey, where you will declare the paper of choice and the specific output(s) on which you will focus for the remainder of the exercise. This step may also involve writing a brief 1-2 page summary of the paper. In Stage 2: Assessment, you will inspect the paper’s reproduction package (raw data, analysis data, and code), connect the output to reproduce with its inputs, and assign a reproducibility score to each output. All this information will be recorded in a standarized spreadsheet. In Stage 3: Improvement, reproducers will be asked you to record specific improvements and report potential changes in the level of reproducibility. Results from stage 2 and 3 will be recorded in a second survey. In a third and final survey for Stage 4: Robustness Checks reproducers will help to track the different analytical choices and test possible variations. References "],
["scoping.html", "Chapter 1 Scoping 1.1 Select a candidate paper or be assigned one. 1.2 Check records on the ACRE platform. 1.3 Verify that a reproduction package exists 1.4 Read and summarize the paper. 1.5 Declare scope of the exercise.", " Chapter 1 Scoping In this stage, you will choose a paper to reproduce or be assigned one, verify that the paper has some reproduction materials, and if materials are available, you will read the paper and declare the scope of the reproduction exercise. Use Survey 1 to record your work for this stage. In this stage, you are not expected to review the reproduction materials in detail, as you will dedicate most of your time to this in the later stages of the exercise. You can expect to spend between 1-3 days in the Scoping stage. 1.1 Select a candidate paper or be assigned one. Ideally, you should select your candidate paper without previously exploring the availability of materials. In the following sections (1.2 and 1.3) these guidelines will show you how to inspect the feasibility of starting a reproduction exercise in your candidate paper. If there are not enough materials to pursue a reproduction, you will be asked to create a short entry in the ACRE platform (&lt;15 minutes) and select a new candidate paper. Repeat this step until you have found a paper with a reproduction package – this will be your declared paper and you will try to reproduce it in the remainder of the ACRE exercise. Do not invest time in doing a detailed read of any paper until you are sure that it is your declared paper. 1.2 Check records on the ACRE platform. Check the ACRE database for previous assessments of your candidate paper. If there are previous entries, you will see a brief report card with the following information: Title: Sample Title Authors: Jane Doe &amp; John Doe Original Reproduction Package Available: URL/No [If “No”] Contacted Authors?: Yes/No [If “Yes(contacted)”] Type of Response: Categories (6). Additional Reproduction Packages: Number (eg., 2) Authors Available for Further Questions for ACRE Reproductions: Yes/No/Unknown Open for reproductions: Yes/No 1.3 Verify that a reproduction package exists We will define a reproduction package (it may also be referred to as a “replication package”) as the collection of all the materials that make it possible for a reproducer to reproduce the paper. Such package may contain data, code, and/or documentation. At this point you are only validating the existence of one (or more) reproduction packages and will not be assessing the quality of its content. When the materials are provided in the original publication, they will be labeled as “the original reproduction package”. If subsequently created by a reproducer, they will be referred to as “reproducer X’ reproduction package”. Verify that the paper has a reproduction package that contains at least some materials. If there is no reproduction package, use our records to check if requests to authors have been made before asking for a reproduction package. If nobody has yet submitted a similar request, you can contact the authors using language suggested here. Alternatively, you can simply choose a different paper and record the candidate paper at the beginning of Survey 1. Contacting the original authors to request a reproduction package. We strongly encourage reproducers to contact authors several weeks before officially starting the Scoping stage of this exercise. Instructors should also plan the use of ACRE exercise accordingly. Before contacting the authors, we recommend that you review and use language from Chapter 6: Communications guidance below. Once you have contacted the authors, please record your interaction using this survey [ADD LINK]. If the authors provide any materials, deposit them in a trusted repository (for example Open Science Framework, Open ICPSR, or Dataverse) and deposit all these materials under the name Original reproduction materials for - Title of the paper. Provide the URL of the repository in Survey 1. If you fail to to obtain the reproduction package and would like to choose a different paper, please use Survey 1 to record this in the “abandoned reproduction” secton of Survey 1. Now that you have verified the existence of a reproduction package, your candidate paper is now your declared paper. You can now move foreard with the exercise! View Decision Tree To Select Paper 1.4 Read and summarize the paper. Depending on how much time you have, we recommend that you write a short summary of the paper. This will help remind you of the key elements to focus on for the reproduction, and to demonstrate your understanding of the paper (for yourself and potentially for the original author). When reading/summarizing the paper, you should try to answer the following main questions: Would you classify the paper’s scientific claims as mainly focused on estimating a causal relationship, estimating/predicting a descriptive statistic of a population, or something else? How many scientific claims (descriptive or causal) are investigated in the paper? What is the population for which the estimates apply? What is the main population that is the focus of the paper as a whole? What are the main data sources used in the paper? How many outputs are in the paper (tables, figures and inline results)? What is the main statistical or econometric method used to examine each claim? What is the author’s preferred specification (or yours if authors are not clear)? What are some robustness checks to the preferred specification? 1.5 Declare scope of the exercise. By now you should have selected a paper that contains a reproduction package, and have a fairly good understanding of the content of the paper. You need not, however, have spent any time reviewing the reproduction package in detail. At this point, you should clearly specify which part of the paper will be the main focus of your reproduction excercise1. You are asked to focus on specific estimates, represented by a unique combination of claim-output-specification, that represented the authors’ preferred specification for a given claim (or yours). If you are planning to scope more than one claim, we strongly advise to start with one and record your results. Then you can initiate another record in ACRE for the second ( third, or more) claim to reproduce, and use the materials and knowledge you developed in the first exercise. Declare specific main estimates to reproduce. Identify one of the scientific claims, and their corresponding preferred specification, and recorded its magnitude, standard error, and location in the paper (page, table #, and row and column in the table). If the authors did not explicitly choose a particular estimate, you will be asked to select one. In addition to the preferred estimate, you will be asked to reproduce up to five estimates that correspond to alternative specifications of the preferred estimate. Declare possible robustness checks to main estimates (optional). After reading the paper you might have wonder why the authors did not conduct a specific robustness test. If you think that such analysis could have been done within the same methodology, and using the same data (eg., including/excluding a subset of the data like “high-school dropouts” o “women”), please specify a robustness test that you would like to test before starting the assessment stage. This concludes all the elements that you will need to conduct the scoping stage. You now have all the elements necesary to complete Survey 1. Bonus: Identify your relevant timeline. Before you begin working on the three main stages of the reproduction exercise (assessment, improvement, and robustness), it is important to manage expectations (yours and those of your Instructor/Advisor). Be mindful of your time limitations when defining the scope of your reproduction activity. This will be given by the type of exercise chosen by the instructor/academic advisory. These will vary from a short format of a homework over a couple of weeks, to a medium format corresponding to a longer class project that that may take a month to complete, long format of a semester-long project (for example as an undergraduate thesis). Table 1 shows tentative distribution of time across three different reproduction formats. The scoping and assessment stages are expected to last roughly the same across all formats (with some additional day for the semester long format, foreseeing less experience with research if the reproducer is an undergraduate). Difference emerge in distribution the time for the last two main stages: Improvements and Robustness. For exercises of short duration, we recommend to stay away from any possible improvements to the raw data (or cleaning code), this will intern limit the amount of robustness check (for example by not being able to reconstruct variable according to slightly different definitions), but it should leave plenty of space for testing different specifications at the analysis level. .tg {border-collapse:collapse;border-spacing:0;} .tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg .tg-baqh{text-align:center;vertical-align:top} .tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top} .tg .tg-dvpl{border-color:inherit;text-align:right;vertical-align:top} .tg .tg-0lax{text-align:left;vertical-align:top} 2 weeks (~10 days) 1 month (~20 days) 1 semester (~100 days) analysis data raw data analysis data raw data analysis data raw data Scoping 10% (1 day) 5% (1 day) 3% (3 days) Assessment 25% 15% 7% Improvement 20% 0% 40% 20% 30% Robustness 30% 10% 30% 30% Extension 5% 0% 10% 10% Paper on PBR. Repro package Later in the assessment stage you will be asked to identify all the outputs that are part of your assessment.↩ "],
["assessment.html", "Chapter 2 Assessment 2.1 Describe the inputs. 2.2 Connect each output to all its inputs 2.3 Assign a reproducibility score.", " Chapter 2 Assessment The goal of this stage is to conduct a standardized assessment of your chosen paper’s computational reproducibility. This stage is designed to record as much of the learning process behind a reproduction as possible. Such a record facilitates future incremental improvements, allowing new reproducers to pick up where others have left off. First, you will provide a detailed description of the reproduction package. Second, you will connect the outputs you’ve chosen to reproduce with their corresponding inputs. With these elements in place, you can score the level of reproducibility of each output, and report on paper-level dimensions of reproducibility. Use Survey 2 to record your work as part of this step. Tip: We recommend that you first focus on one specific output (e.g. “Table 1”). After completing the assessment for this output, you will have a much easier time translating improvements to other outputs. 2.1 Describe the inputs. This section explains how to list all the input materials found or referred to in the reproduction package. First, you’ll identify data sources and connect them with their raw data files (when available). Second, you’ll locate and provide a brief description of the analytic data files. Finally, you’ll locate, inspect, and describe the analytic code used in the paper. The following terms will be used in this section: Cleaning code: A script associated primarily with data cleaning. Most of its content is dedicated to actions such as deleting variables or observations, merging data sets, removing outliers, or reshaping the structure of the data (from long to wide or vice versa). Analysis code: A script associated primarily with analysis. Most of its content is dedicated to actions such as running regressions, running hypothesis tests, computing standard errors, and imputing missing values. 2.1.1 Describe the data sources and raw data. In the paper to be reproduced, find references to all data sources used in the analysis. A data source is usually described in narrative form. For example, if the body of the paper uses text like “…for earnings in 2018 we use the Current Population Survey…”, the data source is “Current Population Survey 2018”. If it is mentioned for the first time on page 1 of the Appendix, its location should be recorded as “A1”. Do this for all data sources mentioned in the paper. Data sources also vary by unit of analysis, with some sources matching the same unit of analysis used in the paper (as in previous examples), while others are less clear (e.g., “our information on regional minimum wages comes from the Bureau of Labor Statistics.” This should be recorded as “regional minimum wages from the Bureau of Labor Statistics”.). Next look at the reproduction package and map the data sources mentioned in the paper to the data files in the available materials. Record their folder locations relative to the main reproduction folder2. In addition to looking at the existing data files, we recommend you review the first lines of all code files (especially cleaning code), looking for lines that call the data sets. Inspecting these scripts may help you understand how different data sources are used, and possibly identify any files missing from the reproduction package. Record this information in a standardized spreadsheet with the following structure: Raw data information: |----------------------|------|-----------------------------------------------|---------------------|---------------------| | data_source | page | data_files | known_missing | directory | |----------------------|------|-----------------------------------------------|---------------------|---------------------| | &quot;Current Population | A1 | cepr_march_2018.dta | | \\data\\ | | Survey 2018&quot; | | | | | |----------------------|------|-----------------------------------------------|---------------------|---------------------| | &quot;DHS 2010 - 2013&quot; | 4 | nicaraguaDHS_2010.csv; | boliviaDHS_2011.csv | \\rawdata\\DHS\\ | | | | boliviaDHS_2010.csv; nicaraguaDHS_2011.csv; | | | | | | nicaraguaDHS_2012.csv; boliviaDHS_2012.csv; | | | | | | nicaraguaDHS_2013.csv; boliviaDHS_2013.csv | | | |----------------------|------|-----------------------------------------------|---------------------|---------------------| | &quot;2017 SAT scores&quot; | 4 | Not available | | \\data\\to_clean\\ | |----------------------|------|-----------------------------------------------|---------------------|---------------------| | ... | ... | ... | ... | ... | |----------------------|------|-----------------------------------------------|---------------------|---------------------| 2.1.2 Describe the analytic data sets. List all the analytic files you can find in the reproduction package, and identify their locations relative to the main reproduction folder. Record this information in a standardized spreadsheet. As you progress through the exercise, add to the spreadsheet a one-line description of each file’s main content (for example: all_waves.csv has the simple description data for region-level analysis). This may be difficult in an initial review, but will become easier as you go along. The resulting report will have the following structure: Analysis data information: |----------------|-----------------------|--------------------------------| | analysis_data | location | description | |----------------|-----------------------|--------------------------------| | final_data.csv | /analysis/fig1/ | data for figure1 | |----------------|-----------------------|--------------------------------| | all_waves.csv | /final_data/v1_april/ | data for region-level analysis | |----------------|-----------------------|--------------------------------| | ... | ... | ... | |----------------|-----------------------|--------------------------------| 2.1.3 Describe the code scripts. List all code files that you find in the reproduction package and identify their locations relative to the master reproduction folder. Review the beginning and end of each code file and identify the inputs required to successfully run the file. Inputs may include data sets or other code scripts that are typically found at the beginning of the script (e.g., load, read, source, run, do ). For each code file, record all inputs together and separate each item with “;”. Outputs may include other data sets, figures, or plain text files that are typically at the end of a script (e.g., save, write, export). For each code file, record all outputs together and separate each item with “;”. Provide a one-line description of what each code file does. Record all this information in the standardized spreadsheet with the following structure: |-------------------|------------------|---------------------|---------------------|----------------------|--------------| | file_name | location | inputs | outputs | description | primary_type | |-------------------|------------------|---------------------|---------------------|----------------------|--------------| | output_table1.do | /code/analysis/ | analysis_data01.csv | output1_part1.txt | produces first part | analysis | | | | | | of table 1 | | | | | | | (unformatted) | | |-------------------|------------------|---------------------|---------------------|----------------------|--------------| | data_cleaning02.R | /code/cleaninig/ | admin_01raw.csv | analysis_data02.csv | removes outliers | cleaning | | | | | | and missing vals | | | | | | | from raw admin data | | |-------------------|------------------|---------------------|---------------------|----------------------|--------------| | ... | ... | ... | ... | ... | ... | |-------------------|------------------|---------------------|---------------------|----------------------|--------------| As you gain an understanding of each code script, you will likely find more inputs and outputs – we encourage you to update the standardized spreadsheet. Once finished with the reproduction exercise, classify each code file as analysis or cleaning. This subjective assessment should be made based on your interpretation of each script’s main role. 2.2 Connect each output to all its inputs With all the information collected above, you can trace your output to reprodue to its primary sources. Upload the standarized table you build to the decribe the code above into the ACRE workflow diagram builder. 2.2.1 Complete workflow information If you were able to identify all the relevant components in the previous section, the ACRE builder will produce and diagram like the following. table 1 └───[code] formatting_table1.R ├───output1_part1.txt | └───[code] output_table1.do | └───[data] analysis_data01.csv | └───[code] data_cleaning01.R | └───[data] survey_01raw.csv └───output1_part2.txt └───[code] output_table2.do └───[data] analysis_data02.csv └───[code] data_cleaning02.R └───[data] admin_01raw.csv The ACRE diagram builder will also generate a table that reprents the diagram above in a data set with the following structure: Data representation of diagram behind Table 1. |--------|-------|-------------------|---------------------|------------| | ouput | order | component | depends_on | inpt_type | |--------|-------|-------------------|---------------------|------------| | table1 | 1 | table1 | formatting_table1.R | code | |--------|-------|-------------------|---------------------|------------| | table1 | 2 |formatting_table1.R| output1_part2.txt | output | |--------|-------|-------------------|---------------------|------------| | table1 | 3 |formatting_table1.R| output1_part1.txt | output | |--------|-------|-------------------|---------------------|------------| | table1 | 4 | output_table1.do | analysis_data01.csv | data | |--------|-------|-------------------|---------------------|------------| | ... | ... | ... | ... | ... | |--------|-------|-------------------|---------------------|------------| 2.2.2 Incomplete workflow information If there are some components missing, then… [Joel and Fernando need to workshop a better solution, until then…] Draw diagrams connecting outputs to raw data sources. To do this, find the code script that generates the target output (formatted or not), then find all inputs required to execute the script (including data and other code files). Repeat until you reach the raw data or the last available file. When a connection cannot be drawn due to a missing component, use the “-||-” symbol. For more examples of diagrams connecting final output to initial raw data, see here. This diagram can be represented in data format by specifying how each component depends on its inputs. For example: Record this information in the standardized spreadsheet. If you have difficulty translating the diagram into a spreadsheet, you can draw it with pen and paper, take a picture and upload it to the assessment survey. 2.3 Assign a reproducibility score. Once all possible inputs have been identified, and there is a clear understanding of the connection between the outputs and inputs, you can start to assess the output-specific level of reproducibility. The following concepts will be used in this section: Computationally Reproducible from Analytic data (CRA): The output can be reproduced with minimal effort starting from the analytic data sets. Computationally Reproducible from Raw data (CRR): The output can be reproduced with minimal effort from the raw data sets. Minimal effort: Five minutes or less are required to run the code, not including computing time. 2.3.1 Levels of Computational Reproducibility for a Specific Output Here we outline different levels of computational reproducibility. Each level is defined by availability of data and materials, and whether or not some type of reproducibility is achieved. In addition, for each level we briefly describe the type of improvements that can be done to improve the current reproducibility. In the next chapter we’ll describe each possible improvement in more detail. The assessment is made at the output level – a paper can be highly reproducible for its main results, but suffer from low reproducibility for other outputs. The assessment is made using a 10-point scale, where 0 represents that, under current circumstances, reproducers cannot access any reproduction package, while 10 represents access to all the materials and that the target outcome can be reproduced starting from raw data. Level 1 (L1): No data or code are available. Possible improvements include adding: raw data (+AD), analysis data (+RD), cleaning code (+CC), and analysis code (+AC). Level 2 (L2): Code scripts are available, but no data are available. Possible improvements include adding: raw data (+AD) and analysis data (+RD). Level 3 (L3): Analytic data are available, but raw data and code are not. Possible improvements include adding: raw data (+RD) and analysis code (+AC). Level 4 (L4): Analytic data sets and analysis code are available. However, code does not run or produces different results than those in the paper (not CRA). Possible improvements include obtaining raw data (+RD) or debugging the analysis code (DAC). Level 5 (L5): Analytic data sets and analysis code are available. They produce the same results as presented in the paper (CRA). The reproducibility package may be improved by obtaining the original raw data sets, or by documenting the steps required to obtain those files. Level 6 (L6): Analytic data and raw data are available. However, some or all cleaning and analysis code are missing. Possible improvements include: adding analysis code (+AC) and/or cleaning code (+CC). Level 7 (L7): All data and analysis code are available. However, the code does not run or produces different results than those presented in the paper (not CRA). Possible improvements include: adding the missing cleaning code (+CD) or debugging the analysis code (DAC). Level 8 (L8): All data and analysis code are available, but cleaning code may be missing. They produce the same results as presented in the paper (CRA). The reproducibility package may be improved by adding the missing cleaning code (+CD). Level 9 (L9): All materials (raw data, analytic data, cleaning code, and analysis code) are available. However, the code does not run or produces different results than those presented in the paper (not CRR and not CRA). Possible improvements include: debugging the cleaning code (DCC) or debugging the analysis code (DAC). Level 10 (L10): All materials (raw data, analytic data, cleaning code, and analysis code) are available. The analysis code produces the same output as presented in the paper (CRA). However, the cleaning code does not run or produces different results that those presented in the paper (not CRR). Possible improvements include: debugging the cleaning code (DCC). Level 11 (L11): All materials are available and produce the same results as presented in the paper with minimal effort, starting from the analytic data (yes CRA) or the raw data (yes CRR). The following figure summarizes the different levels of computational reproducibility (for any given output). For each level, there will be improvements that: have been made (-), can be made to move up one level of reproducibility (✔), or are out of reach given the current level of reproducibility (x). Figure 1: Levels of Computational Reproducibility and Possible Improvements | Level | Possible improvements | |----------------------------------------------------------| | |+Analysis| +Raw |+Analysis|+Cleaning|Debug|Debug| | | Data | Data | Code(AC)| Code(CC)| AC | CC | --------|---------|--------|---------|---------|-----|-----| Data and code available ? | | | | | | | | ├── None ..................................| L1 | ✔ | ✔ | ✔ | ✔ | x | x | ├── Some code only.........................| L2 | ✔ | ✔ | x | x | x | x | └── What data are available? | | | | | | | | ├── Analytic data only. Code? | | | | | | | | | ├── No code or cleaning code only..| L3 | - | ✔ | ✔ | ✔ | x | x | | └── Analysis code only. Is it CRA? | | | | | | | | | ├── No..........................| L4 | - | ✔ | - | ✔ | ✔ | x | | └── Yes.........................| L5 | - | ✔ | - | ✔ | - | x | └── Raw &amp; Analytic data. Code? | | | | | | | | ├── None ...........................| L6 | - | - | ✔ | ✔ | x | x | ├── Analysis code only. CRA? | | | | | | | | | ├── No...........................| L7 | - | - | - | ✔ | ✔ | x | | └── Yes..........................| L8 | - | - | - | ✔ | - | x | └── A. and C. code. Is it CRR? | | | | | | | | ├── No. CRA? | | | | | | | | | ├── No.......................| L9 | - | - | - | - | ✔ | ✔ | | └── Yes......................| L10 | - | - | - | - | - | ✔ | └── Yes.........................| L11 | - | - | - | - | - | - | Choose the appropriate level of computational reproducibility and record it using the following format. |-------------|-------|------------------------|------------| | output_name | level | additional_explanation | other_info | |-------------|-------|------------------------|------------| | table 1 | 4 | | ... | |-------------|-------|------------------------|------------| | table 2 | 7 | | ... | |-------------|-------|------------------------|------------| | figure 1 | 5 | | ... | |-------------|-------|------------------------|------------| | ... | ... | ... | ... | |-------------|-------|------------------------|------------| Record this information in the standarized spreadsheet. You will be asked to provide this information in the assessment and improvement survey. To present each category above as levels that reflect a gradient of improvement, we need to make explicit the following underlying valuations. L1: The worst is to have no access to any data or code. L2: Having only some code is better than nothing, but worst than any data. L3: Having analytic data but not code is better than having code and no data. L4 and L5: Conditional on having all analytic material. Reproducible is better than not L6: Having raw and analytic data, even without any code, is better than having CRA. L7: Given that there is raw and analysis data, having a cleaning code only does not add much. having analysis code does add (to lvl 7 and lvl 8 if CRA) L9: Having all materials that but not being able to reproduce is better than all previous cases. Achieving CRA (lvl 10) and CRR (lvl 11) are the best levels according to these criteria. The reproducer and/or reader of this information can disagree with this subjective valuations. In this scenario, the levels should be understood as unordered categories, or ordered under different criteria. The reader of these guidelines are also encouraged to submit suggested edits using the ADD IMAGE button above. 2.3.2 Reproducibility dimensions at the paper level In addition to an output-specific assessment of computational reproducibility, several practices can facilitate a paper’s overall computational reproducibility. These practices are described in detail in the Improvement chapter. In the Assessment section, it is only required that you verify whether the original reproduction package made use of any of the following: master script that runs all steps readme file standard file organization version control analysis uses open source software dynamic document computing capsule (e.g. CodeOcean, Binder, etc.) Congratulations! You have now completed the Assessment stage. You have provided a concrete building block of knowledge to improve understanding of the state of reproducibility in Economics. Please continue to the next section where you can help to improve it! relative location takes the form /folder_in_rep_materials/sub_folder/file.txt, in contrast to an absolute location that has the form username/documents/projects/repros/folder_in_rep_materials/sub_folder/file.txt↩ "],
["improvements.html", "Chapter 3 Improvements 3.1 Types of output-level improvements 3.2 Types of paper-level improvements", " Chapter 3 Improvements After completing an assessment of the paper’s reproducibility packages, you can start proposing ways to improve it’s reproducibility. Making improvements provides an opportunity to gain deeper understanding of the paper’s methods, findings, and overall contribution. Each contribution can also be assessed and used by the wider ACRE community–other students and researchers using the ACRE platform around the world. As in the Assessment section, we recommend that you first focus on one specific output (e.g., “Table 1”). After making improvements to this first output, you will have a much easier time translating those improvements to other outputs. Use Survey 2 to record your work as part of this step. 3.1 Types of output-level improvements 3.1.1 Adding missing raw data files or meta-data (+RD) Reproduction packages often do not include all original raw datasets. To obtain any missing raw data, or information about them, follow these steps: Identify a specific missing file. During Assessment, you identified all data sources from the paper’s body and appendices (column data_source in this standarized spreadsheet). However, some data sources (as collected by the original investigators) might be missing one or more data files. You can sometimes find the specific name of those files by looking at the beginning of the cleaning code scripts. If you find the name of the file, record it in the data_file field of the same spreadsheet as above. If not, recorded it as “Some (or all) of the files used in the paper corresponding to data source X”. Verify whether this file (or files) can be easily obtained from the web. 2.1 - If yes: obtain the missing files and add them to the reproduction package. Make sure to obtain permission from the origina author to repost this data. See tips for communication for a template email. 2.2 - If no: proceed to step 3. Use the ACRE database to verify whether there have been previous attempts to contact the authors regarding this paper. Contact the original authors and politely request the original materials. Be mindful of the authors’ time, and remember that the paper you are trying to reproduce was possibly published at a time when standards for computational reproducibility were different. See tips for communication for sample language on how to approach authors. If the data sets are not available due to confidentiality or proprietary issues, you can still improve the reproduction package by providing detailed instructions for future researchers to follow, including contact information and possible costs. In addition to trying to obtain raw data, you can also contribute by obtaining missing analytic data. 3.1.2 Adding missing analytic data files (+AD) Analytic data can be missing for two reasons: (i) raw data exists, but the procedures to transform it into analytic data are not fully reproducible, or (ii) some or all raw data is missing and some or all analytic data is not included in the original reproduction package. To obtain any missing analytic data, follow these steps: Identify the specific name of the missing data set. Typically this information can be found in some of the analysis code that calls the data to perform an analysis (eg analysis_data_03.csv). Verify that the data cannot be obtained by running the data cleaning code over the raw data. Use the ACRE database to verify if previous attempts have been made to contact the authors about this data. Contact the authors and request the specific data set. 3.1.3 Adding missing analysis code (+AC) Analysis code can be added when analytic data files are available, but some or all methodological steps are missing from the code. In this case, follow these steps: Identify the specific line or paragraph in the paper that describes the analytic step that is missing from the code (e.g., “We impute missing values to…” or “We estimate this regression using a bandwidth of…”). Identify the code file and the approximate line in the script where the analysis can be carried out. If no relevant code file is found, identify the location of the missing file relative to the steps in the reproduction diagram. Use the ACRE database to verify if previous attempts have been made to contact the authors about this issue. Contact the authors and request the specific code files. If the authors don’t respond, we encourage you to attempt to recreate the analysis using your own interpretation of the paper, and making explicit your assumptions when filling in any gaps. 3.1.4 Adding missing data cleaning code (+CC) Data cleaning (processing) code might be added when steps are missing in the creation or re-coding of variables, merging, subsetting of the data sets, or other steps related to data cleaning and processing. You should follow the same steps you used when adding missing analysis code (1-5). 3.1.5 Debugging analysis code (DAC) Whenever code is available in the reproduction package, you should be able to debug those scripts. There are four types of debugging that can improve the reproduction package: Code cleaning: Simplify the instructions (e.g., by wrapping repetitive steps in a function or a loop) or remove redundant code (i.e., old code that was commented out) while keeping the original output intact. Performance improvement: Replace instructions with new ones that perform the same tasks but take less time (e.g., choose one numerical optimization algorithm over another, but obtain the same results). Environment set up: Modify the code to include correct paths to files, specific versions of software, and instructions to install missing packages or libraries. Correcting errors: A coding error will occur when a section of the code in the reproduction package executes a procedure that is in direct contradiction with the intended procedure expressed in the documentation (i.e., paper or code comments). For example, an error will happen if the paper specifies that the analysis is performed on a population of males, but the code restricts the analysis to females only. Please follow the ACRE procedure to report coding errors. 3.1.6 Debugging cleaning code (DCC) Use the same steps as you took to debug the analysis code, but report them separately. 3.1.7 Reporting results Track all the different types of improvements you make and record in this standarized spreadsheet with the following structure: Level-specific quality improvements: add data/code, debug code. | output_name | imprv | description_of_added_files | lvl | |-------------|-------|-----------------------------------|-----| | table 1 | +AD | ADD EXAMPLES | 5 | | table 1 | +RD | ADD EXAMPLES | 5 | | table 1 | DCC | ADD EXAMPLES | 5 | | figure 1 | +CC | | 6 | | figure 1 | DAC | | 6 | | inline 1 | DAC | | 8 | | ... | ... | ... | ... | 3.2 Types of paper-level improvements There are at least six additional improvements you can make to improve a paper’s overall reproducibility. These additional improvements can be applied across all reproducibility levels (including level 10). Set up the reproduction package using version control software, such as Git. Improve documentation by adding extensive comments to the code. Integrate the documentation with code by adapting the paper into a literate programming environment (e.g., using Jupyter notebooks, RMarkdown, or a Stata Dynamic Doc). If the code was written using a proprietary statistical software (e.g., Stata or Matlab), re-write it using an open source statistical software (e.g., R, Python, or Julia). Re re-organize the reproduction package into a set of folders and sub-folders that follow standardized best practices, and add a master script that executes all the code in order, with no further modifications. See AEA’s reproduction template. Set up a computing capsule that executes the entire reproduction in a web browser without needing to install any software. For examples, see Binder and Code Ocean. 3.2.1 Reporting improvements You will be asked to provide this information in the Assessment and Improvement Survey. "],
["robust.html", "Chapter 4 Checking for Robustness 4.1 Identifying Analytical Choices 4.2 Identifying Choice Type 4.3 Identifying Analytical Choices 4.4 Identifying Analytical Value 4.5 Choose and justify alternative values for analytical choices 4.6 Test the robustness of results", " Chapter 4 Checking for Robustness [UNDER CONSTRUCION] Identify all possible analytical choices: original and repeated ones. Identify type of choice. Identify choice value. Suggest choice alternative and justify (one line) 4.1 Identifying Analytical Choices As part of the requirements to demonstrate comprehension of the paper and the code researchers conducting the reproduction will be asked to record all the analytical choices identified during the code review process. This is done in two steps: first adding comment lines into the code files where an analytic choice are found, and second, compiling those analytic choices into a standardized data set. In your copy of the replication code, add the comment “# ANALYTICAL CHOICE OF TYPE ____. RECORDED FOR THE FIRST TIME [HERE or IN &quot;FILE_NAME-LINE_NUMBER&quot;]” above each analytical choice detected in the code. Possible types of analytical choices include (but are not limited to): Analytical choices in data cleaning code: Variable definition Data sub-setting Data reshaping (merge, append, long/gather, wide/spread) Others (specify as “processing - other”) Analytical choices in analysis code: Regression function (link function) Key parameters (tuning, tolerance parameters, etc.) Controls Adjustment of standard errors Choice of weights Treatment of missing values Imputations Other (specify as “methods - other”) Once finished, transcribe all the information on analytical choices into a data set. For the source field type “original” whenever the analytical choice is identified for the first time, and file_name-line number every time that the same analytical choice is applied subsequently (for example if a analytic choice is identified for the first time in line 103 and for a second in line 122 their respective values for the source field should be original and code_01.do-L103 respectively). The resulting data base should have the following structure: file_name line_number choice_type choice_value Source code_01.do 73 data subsetting males original code_01.do 122 variable definition income = wages + capital gains “code_01.do-L103” code_05.R 143 controls age, income, education original … … … … … 4.2 Identifying Choice Type 4.3 Identifying Analytical Choices 4.4 Identifying Analytical Value 4.5 Choose and justify alternative values for analytical choices 4.6 Test the robustness of results Test the robustness of results to alternative (sensible) specifications Identify sensible alternatives to analytical choices. Sample from sensible analytical choices and re-run: report how much do results change as fraction of standard deviations. Jackknife the preferred estimate. Use ML to select among covariates… "],
["concluding-the-reproduction.html", "Chapter 5 Concluding the reproduction 5.1 Final products", " Chapter 5 Concluding the reproduction [UNDER CONSTRUCTION] but testing now Walk the students on checking that they have completed all the steps and where can they see their output. 5.1 Final products One-page introduction describing why you chose this paper Two-page summary of paper 2 Completed surveys: i - General information about the paper and specific information about output to reproduce. ii - Assessment of how (computationally) reproducible is the paper; description of improvements to its reproducibility; record of all the analytical choices identified in the exercise. ACRE report card with all the improvements that were created by the researcher reproducing the paper. The list of improvements will be made public and original authors will receive a copy of the report card. The option of anonymity will be provided to the researchers reproducing the paper. New Readme file (autogenerated). Data with all analytical choices identified. ?? Narrated description of improvements to original CR of the paper, assessment of robustness of results. Lessons from the exercise and possible future extensions. "],
["guidance-for-a-constructive-exchange-between-reproducers-and-original-authors.html", "Chapter 6 Guidance for a Constructive Exchange Between Reproducers and Original Authors 6.1 For Reproducers Contacting the Authors of the Original Study", " Chapter 6 Guidance for a Constructive Exchange Between Reproducers and Original Authors The purpose of this chapter is to facilitate constructive and respectful communication between reproducers (you) and original authors. Exchanges that contain charged or adversarial language can damage professional relationships and hamper scientific progress. Janz and Freese (2019) articulate two important steps reproducers can take to ensure their interactions with original authors are constructive. We provide a summary below and encourage you to follow this guidance. Remember the golden rule of reproductions (and replications): treat others and their work, as you would like others to treat you and your work! 1. Carefully and transparently plan your study. Clearly state that you are conducting a reproduction of the original work. Explain why you have chosen this study. Try to proactively address potential concerns for selection bias. Explain how “far” your results must deviate from the original work before claiming that the study could not be reproduced. Engage deeply with the substantive literature to ensure that your interpretation of differences between the original and reproduction is thorough and acceptable to other authors in the field. 2. Use professional and sensitive language. Discuss potential discrepancies between your work and the original paper just like you would have done for your own work. Avoid binary judgments and statements like “failed to reproduce.” Clearly state which results reproduced and which did not (e.g., “we successfully reproduced X, but failed to reproduce Y”) unless you uncover apparent scientific misconduct (e.g. see Broockman, Kalla and Aronow, 2015). Talk about the study, not the author, to avoid making it personal. Make clear what the positive contribution of the original article is. Consider sending a copy of your reproduction report to the original authors. Discuss what your reproduction contributes to the literature, and refrain from claiming to give the final answer to the question. For papers published five or more years ago, be mindful that norms for reproducibility have evolved since then. Remember, the goal is not to criticize previous work or hunt for errors, but to move the literature forward! To help put these recommendations into practice, we provide template language for common scenarios that reproducers and authors may encounter in their interactions. While we hope that you find these useful, note that they are only recommendations, and you are welcome to modify them based on the context and needs of your specific project. Feel free to contact us if you need more guidance or would like to provide feedback on these materials. 6.1 For Reproducers Contacting the Authors of the Original Study Consider the following before you contact the original author: Carefully read all footnotes, appendices, tables, captions, etc. to learn if, how, and where reproduction materials are provided. Follow this Data and Code Guidance to determine whether you have everything before you start. A few things to consider: A Readme file, if available, would be a good place to start. All papers published in AEA journals after July 2019 should have such documentation. Check whether there are any restrictions to accessing the data or code, and whether there are instructions on how to access these files for the purpose of reproduction. If reproduction materials are not readily available in the location where the article is published (e.g., the journal website), check the authors’ websites, Dataverse profiles, the ICPSR Publications Related Archive, and other relevant archives and/or data repositories. If steps 1 and 2 don’t work, contact the corresponding author (copying the co-authors, if any), consolidating your requests into as few emails as possible. In your email, make sure to include the following details: Basic information about the paper being reproduced (include title, version, date, and a DOI link (or just a URL)); Context for the reproduction (as part of a class exercise, thesis, etc.) and a notice that the outcome will be recorded in the ACRE reproducibility database; Items from the reproduction package that are missing, as well as locations where you had (unsuccessfully) searched for them; Use plan: Will the materials be used exclusively for this project? Ask for permission to share the data publicly. Right to consultation and results: Will you share the outcome of the reproduction exercise with the original authors? A deadline to respond (we suggest at least two weeks). Follow up if you don’t get a response within two weeks (or whatever deadline you set), and include any details or clarifications that were left out in your first email. 6.1.1 Contacting the original author(s) to request items that are missing Template email: Subject: Reproduction materials for [“Title of the paper”] Dear Dr. [Lastname of Corresponding Author], I am contacting you about reproduction materials for your paper titled [Title] which was published in [Journal] in [year] (vol [volume], no. [no.]), [link]. I am a [graduate student/postdoc/other position] at [Institution], and I’m working to reproduce this paper as part of a class exercise. [Add context for why you want to reproduce this particular paper using neutral language (e.g., &quot;This is a seminal paper in my field&quot;), avoiding any statements that would put the respondent on the defensive]. To be able to reproduce the paper in full, I hope that you can share the following items: [list items missing from reproduction package, preferably bulleted if more than one (raw/analytic data, code, protocols for conducting the experiment, etc.)]. I have already searched [locations where you searched for items, with links provided], however I was not able to locate the items there. You can be assured that I will not share any of the materials without your permission, and I will use them exclusively for the purpose of this exercise. Let me know if there are any legal or ethical restrictions that apply to all or parts of the reproduction materials so that I can take that into consideration during this exercise. Note that I will record the outcome of my reproduction on the Advancing Computational Reproducibility in Economics (ACRE) platform, an online catalog of reproduction projects in economics. ACRE is hosted by the Berkeley Initiative for Transparency in the Social Sciences (BITSS). Let me know if you would like me to share the outcome of my exercise with you and whether you are interested in providing a response. Since I am required to complete this project by [date], I would appreciate your response by [deadline]. Let me know if you have any questions. Please also feel free to contact my supervisor/instructor [Name (email)] for further details on this exercise. Thank you in advance for your help! Best regards, [Reproducer] 6.1.2 Asking for additional guidance when some materials have been shared Note: Even when a corresponding author has shared a reproduction package, you may still run into challenges in interpreting or executing the materials. That shouldn’t discourage you from asking the corresponding author to provide clarifications or share missing materials. As in the first scenario described above, demonstrate that you made an honest effort to reproduce the work using the available resources and try to consolidate your requests into as few emails as possible. Template email: Subject: Clarification for reproduction materials for [“Title of the paper”] Dear Dr. [Lastname of Corresponding Author], Thank you for sharing the materials. They have been immensely helpful for my work. Unfortunately, I ran into a few issues as I delved into the reproduction exercise, and I think your guidance would be helpful in resolving them. [Describe the issues and how you have tried to resolve them. Describe whatever files or parts of the data or code are missing. Refer to examples 1 and 2 below for more details]. Thank you in advance for your help. Best regards, [Reproducer] 1: An example of well described issues: Specifically, I am attempting to reproduce OUTPUT X (e.g., table 1, figure 3). I found that the following components are required to reproduce to reproduce OUTPUT X: OUTPUT X └───[code] formatting_table1.R ├───output1_part1.txt | └───[code] output_table1.do | └───[data] analysis_data01.csv | └───[code] data_cleaning01.R* | └───[data] UNKNOWN └───output1_part2.txt └───[code] output_table2.do └───[data] analysis_data02.csv └───[code] data_cleaning02.R └───[data] admin_01raw.csv* I have marked with an asterix (*) the items that I could not find in the reproduction materials: data_cleaning01.R and admin_01raw.csv. After accessing these files, I will also be able to identify the name of the raw data set required to obtain output1_part1.txt. This is to let you know that and that I may need to contact you again if I cannot find this file (labeled as UNKNOWN above) in the reproduction materials. I understand that this request will require some work for you or somebody in your research group, but I want to assure you that I will add these missing files to the reproducibility package for your paper on the ACRE platform. Doing this will ensure that you will not be asked twice for the same missing file. 2. An example of poorly described issues: Your paper does not reproduce. I have tried for several hours now, and can’t get the DO files to run. Could you please share all the missing reproduction materials? Data and code sharing are a basic principle of open science, so I am confident that you will do the right thing. 6.1.3 Response when the original author has refused to share due to undisclosed reasons Note: You can also use this template if a corresponding author has not submitted a response after two or more follow-up emails. Template email: Subject: Re: Reproduction materials for `[“Title of the paper”] Dear Dr. [Last Name of Corresponding Author], Thank you for considering my request. I will try to reproduce the paper using the available materials, and will record the missing items accordingly on the ACRE platform. l will also post my assessment of the reproducibility of the paper in its current form based on the ACRE reproducibility scale. Let me know if you have any questions. Best regards, [Reproducer] 6.1.4 Response when the original author has refused to share due to legal or ethical restrictions of the data Template email: Subject: Re: Reproduction materials for `[“Title of the paper”] Dear Dr. [Last Name of Corresponding Author], Thank you for your response and for clarifying the terms of use of the reproduction materials. Though I understand that you are unable to share the raw data, there may be alternative steps you can take that would help me improve the reproducibility of your paper. These include: Sharing the analytic version of the data (the version of the dataset that was used in the final version of your paper); Providing a public description of the steps other researchers can follow to request access to the raw data or materials, including an estimate of the costs and the duration of the process. Find examples of data availability statements for proprietary or restricted-access data here; and Providing access to all data and materials for which the constraints do not apply. Based on my assessment, your paper would currently rank at [level X] on the ACRE reproducibility scale, however this score can be easily improved. Being able to provide analytic data would elevate the reproducibility of your paper to [level Y]. Providing public instructions on how other parties can access the data would further elevate its reproducibility to [level Z]. I would be happy to help if you are interested in taking any of the steps I outlined above. Let me know if that would be helpful. Thank you for your help! Best regards, [Reproducer] 6.1.5 Contacting the original author to share the results of your reproduction exercise Note: Reporting the results of reproductions is probably the most contentious part of the process, particularly in instances where the reproducer is not able to fully reproduce the paper or finds significant deviations from the original work. However, if the reproduction can correctly identify the sources of such deviations, it may be viewed as an improved version of the original work. Regardless of the outcome of the reproduction exercise, the guidance from the introduction of this chapter still stands here: reproduce the work of others as you would like for others to reproduce yours, and make sure that is reflected in how you discuss any discrepancies between your and the original work. Template email: &gt;Subject: Reproducibility Assessment of Dear Dr. [Last Name of Corresponding Author], Thank you for your support throughout my project as I worked to verify and advance the reproducibility of [Paper]. I’m writing now to share the results of my project with you and invite your feedback. The results of each step of my exercise, include i) Assessment, ii) Improvements, iii) Robustness Checks, (and iv) Extensions, if applicable). [Include the following items in the body of your email: - Briefly describe which parts of the paper you tried to reproduce (e.g. a specific estimate, a table, etc.). - Within the scope of your reproduction, describe exactly which items you managed to reproduce. - Discuss the differences you observed between the results of your reproduction and the original work, and demonstrate that you did your due diligence in trying to reproduce the item. Remember that it is more constructive to discuss discrepancies, differences or deviations, rather than errors, mistakes or failures, and *always talk about the work -- not the author!* - Use sensitive language when presenting discrepancies, e.g. “Unfortunately, I found X, which differs from the Y result in the original paper...”. Be cognizant of any potential limitations of your work, and explain how you have tried to address them -- that way you will proactively address potential criticism! - Describe how you tried to improve, the reproducibility of the paper. If some of the improvements are based on discretionary judgment (e.g. file organization or code commenting), try to explain why you think they are an upgrade over the original work. If you didn’t make improvements, point out some concrete steps that the author(s) can take to improve the reproducibility of the section you reproduced.] I look forward to your questions, comments, and suggestions on what I laid out above. As discussed previously, I will record the outcomes of my exercise, along with the improvements, on the ACRE platform. Best regards, [Reproducer] 6.1.6 Responding to hostile responses from original authors Note: Planning your study carefully and transparently, and using professional and sensitive language are the best ways to ensure that the interaction will be beneficial to both you and the original author. However, unpleasant interactions may happen despite your best efforts, and can range anywhere from dismissive comments to bullying, discrimination, and harassment. 6.1.6.1 Dismissive comments In cases of dismissive comments, the best course of action may be to simply thank the author for their response and continue with the exercise. Template email: Subject: Re: Reproduction materials for `[“Title of the paper”] Dear Dr. [Last Name of Corresponding Author], Thank you for your response. I will work to reproduce using the available materials, and will record my results accordingly on the ACRE platform. l will also post my assessment of the reproducibility of the paper in its current form based on the ACRE reproducibility scale. Let me know if you have any questions. Best regards, [Reproducer] 6.1.6.2 Harassment and/or discrimination The AEA and other economic societies have strict policies against harassment and discrimination. Here are some of the behaviors that the AEA Policy on Harassment and Discrimination has listed as unacceptable, and could emerge in a hostile exchange regarding a reproduction: Intentionally intimidating, threatening, harassing, or abusive actions or remarks (both spoken and in other media) Prejudicial actions or comments that undermine the principles of equal opportunity, fair treatment, or free academic exchange Deliberate intimidation, stalking, or following Real or implied threat of physical harm. Here are a some steps you can take if you believe you have experienced bullying, discrimination or harassment: File a complaint with the AEA Ombudsperson. Any AEA member can file a complaint (you can also join the AEA solely for the purpose of filing a report). The person about whom you are making the complaint need not be an AEA member. A non-AEA member can also file a report if the act of harassment or discrimination was committed by an AEA member or in the context of an AEA-sponsored activity. Learn more about the process here. File a report with your institution’s office for the prevention of harassment &amp; discrimination. US-based institutions have internal mechanisms that allow students and faculty to seek support in cases of discrimination and harassment on the basis of race, color, national origin, gender, age, or sexual orientation/identity, including allegations of sexual harassment and sexual violence. Formal titles of this office vary across institutions, but common names include “Office for the Prevention of Harassment and Discrimination” (in institutions that are part of the University of California system), “Office of Equity and Title IX”, etc. Contact your institution’s Ombudsperson/Ombuds Office. If you believe that you have experienced academic bullying or other forms of disrespectful behavior that fall outside the scope of harassment and/or discrimination as described above, you should know that university ombuds officers are a confidential, impartial resource to discuss your concerns and learn about potential next steps available in your case. Access mental health services at your institution. Many universities offer short-term Counseling &amp; Psychological Services (CAPS) for academic, career, and personal issues. Ask for support from your academic supervisor. If you are unsure on how to proceed, consult your academic supervisor on whether continuing the exercise would be appropriate. "],
["reproduction-diagrams.html", "Chapter 7 Reproduction Diagrams 7.1 Different Scenarios", " Chapter 7 Reproduction Diagrams 7.1 Different Scenarios JOEL: PLease fill-in. 7.1.1 Complete table 1 └───[code] formatting_table1.R ├───output1_part1.txt | └───[code] output_table1.do | └───[data] analysis_data01.csv | └───[code] data_cleaning01.R | └───[data] survey_01raw.csv └───output1_part2.txt └───[code] output_table2.do └───[data] analysis_data02.csv └───[code] data_cleaning02.R └───[data] admin_01raw.csv 7.1.2 Raw data and analytic data, but cleaning code is missing. table 1 └───[code] formatting_table1.R ├───output1_part1.txt | └───[code] output_table1.do | └───[data] analysis_data01.csv | └───[code] MISSING FILE(S) | └───[data] survey_01raw.csv └───output1_part2.txt └───[code] output_table2.do └───[data] analysis_data02.csv └───[code] MISSIN FILE(S) └───[data] admin_01raw.csv "],
["additional-resources.html", "Chapter 8 Additional resources 8.1 Some summaries 8.2 Links", " Chapter 8 Additional resources Coding errors: A coding error will occur when a section of the code, of the reproduction package, executes a procedure that is in direct contradiction with the intended procedure expressed in the documentation (paper or comments of the code). For example an error happens if the paper specify that the analysis is perform on the population of males, but the code restricts the analysis to females only. Please follow the ACRE procedure to report coding errors. Create a section with short summaries of great resources for comp. repro and invite reader to contribute. 8.1 Some summaries 8.1.1 Summary on reproducible workflow (Chapter 11) from Christensen, Freese, and Miguel (2019): TODO 8.2 Links TODO: Add and classify Project TIER IDB’s cheatsheet for transparency, reproducibility and ethics Lars Vilhuber LDI’s Wiki for Reproducibility. Particularly this section. World Bank DIME’s Wiki for transparent and reproducible research. Dynamic documents in R, Python and Stata Git resources: Jenny Bryan’s book and video Github learning lab Udacity’s intro Git for poets Combining GitHub and Dropbox Atlassian intro to Git Software Carpentry tutorial from the command line Open Science Framework (OSF) R for Stata users References "],
["contributions.html", "Chapter 9 Contributions 9.1 Ask for feedback on guidelines 9.2 List of Contributors", " Chapter 9 Contributions TO DO 9.1 Ask for feedback on guidelines 9.2 List of Contributors "],
["definitions.html", "Chapter 10 Definitions 10.1 Basic concepts.", " Chapter 10 Definitions 10.1 Basic concepts. Raw data sets are unmodified files obtained by the authors from the sources cited in the paper. Raw data from which personally identifiable information (PII) has been removed is still considered raw. All other modifications to raw data make it processed. A data set may be classified as raw if it fits any of the following criteria: The data is stored in a folder or file labeled as “raw”. The data set is not the output of any code script in the reproduction package. The same data file can be independently obtained from the data source cited in the paper. Processed data are raw data sets that have gone through any transformation other than the removal of PII. There are two kinds of processed data: Intermediate data are not directly used as final input for analyses presented in the final paper (including appendices). Intermediate data should not contain direct identifiers. Analytic data is used as the final input in a workflow in order to produce a statistic displayed in the paper (including appendices).A data set may be classified as analytic if it fits any of the following criteria: The data is stored in a folder or file labeled as “analytic” or “analysis.” The data set is the last input required to produce some of the output (formatted or unformatted) of the paper. Causal claim: This paper estimates the effect of X on Y for population P, using method F. Example: “This paper investigates the impact of bicycle provision on secondary school enrollment among young women in Bihar/India, using a Difference in Difference approach.” Descriptive/predictive claim: This paper estimates the value of Y (estimated or predicted) for population P under dimensions X using method M. Example: “Drawing on a unique Swiss data set (population P) and exploiting systematic anomalies in countries’ portfolio investment positions (method M), I find that around 8% of the global financial wealth of households is held in tax havens (value of Y)” Reproduction package: Collection of all the materials associated with the reproduction of a paper. A reproduction package may contain data, code and documentation. When the materials are provided in the original publication they will be labeled as ‘original reproduction package’, when they provided by a previous reproducer they will be referred as ‘reproducer X’s reproduction package’. At this point you are only assessing the existence of one (or more) reproduction packages, you are will not be assessing the quality of its content at this stage. "],
["references.html", "References", " References "]
]
