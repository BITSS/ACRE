[
["robust.html", "Chapter 4 Checking for Robustness 4.1 Mapping the universe of robustness checks 4.2 Proposing reasonable alternatives", " Chapter 4 Checking for Robustness Once you have assessed and improved the current computational reproducibility of a paper, it is possible to extend the robustness checks presented in the original paper to a much larger number. We use the term robustness checks to describe any possible change in a computational choice, both in data analysis and data cleaning, and its subsequent effect on the main estimates of interest. The universe of robustness checks can be very large or potentially infinite. The focus should be on the set of reasonable specifications (Simonsohn et. al., 2018), defined as (1) sensible tests of the research question, (2) expected to be statistically valid, and (3) not redundant with other specifications in the set. The extension of new robustness checks will depend on the current level of reproducibility. A claim supported by display items with reproducibility levels of 0 or 1 cannot have any robustness checks in addition to what is already in the paper. A claim with levels 2-4 might be able to perform some robustness checks regarding the analysis, but not the specific estimates declared in Scoping (it is not computationally reproducible from analysis data or CRA). A claim based on display items at level 5 can check the robustness to the core of the analysis as describe in the paper. Finally a claim associated with levels 6-10 will allow for robustness checks to variable definitions and data manipulations. It is important to highlight that the set of feasible robustness checks grows exponentially, as its defined by all the possible combinations of available checks. For example, when checking the robustness to a new variable definition (level 6 and above) reproducers will also have the alternative to test how the main estimate changes under an alternative variable definition and an alternative core analytical choice. Going back to our diagram that represents the multiple parts of a paper (??), the robustness section begins at the claim level. For a given claim, there will be several specifications presented in the paper, one of which is identified by the authors (or yourself in their abscense) as the main or preferred specification. Identify which display item contains this specificaiton and go back to your reproduction tree for this display item, this will give you a list of all the code files where you can potentially modify a computational choice. Using the example tree discussed in the Assessment stage, we can remove the data files for simplicity and obtain the following: table1.tex (contains preferred specification of a given claim) |___[code] analysis.R |___[code] final_merge.do |___[code] clean_merged_1_2.do | |___[code] merge_1_2.do | |___[code] clean_raw_1.py | |___[code] clean_raw_2.py |___[code] clean_merged_3_4.do |___[code] merge_3_4.do |___[code] clean_raw_3.py |___[code] clean_raw_4.py This simplified tree gives you a list of potential files where you could test different reasonable specifications. Here we suggest two types of contributions to robustness checks: contributing to mapping the universe of robustness checks and contributing to testing reasonable specifications. Both contributions should be recorded in the ACRE platform (under development) refering to files in a specific reproduction package. 4.1 Mapping the universe of robustness checks When examining a specific code file, or a specific section within a file, you will be asked to identify and record all specific analytical choices in that file or section. Possible types of analytical choices include (but are not limited to): Analytical choices in data cleaning code: Variable definition Data sub-setting Data reshaping (merge, append, long/gather, wide/spread) Others (specify as “processing - other”) Analytical choices in analysis code: Regression function (link function) Key parameters (tuning, tolerance parameters, etc.) Controls Adjustment of standard errors Choice of weights Treatment of missing values Imputations Other (specify as “methods - other”) Once finished, transcribe all the information on analytical choices into a data set (the ACRE platform will allow for easier recording once deployed). For the source field type “original” whenever the analytical choice is identified for the first time, and file_name-line number every time that the same analytical choice is applied subsequently (for example if a analytic choice is identified for the first time in line 103 and for a second in line 122 their respective values for the source field should be original and code_01.do-L103 respectively). For each analytical choice recorded, add the specific choice that the paper used, and describe what other alternatives could have been used. The resulting data base should have the following structure: entry_id file_name line_number choice_type choice_value choice_range Source 1 code_01.do 73 data sub-setting males males, female, original 2 code_01.do 122 variable definition income = wages + capital gains wages, capital gains, gifts “code_01.do-L103” 3 code_05.R 143 controls age, income, education age, income, education, region original … … … … … … … The advantage of this type of contribution is that you are not requiered to have in depth knowledge of the paper and its methodology to make a contributions. This allows you to potentially map several code files achieving a broader understanding of the paper. The disadvantage is that you are not expected to test alternative specifications. 4.2 Proposing reasonable alternatives When performing the different robustness tests, you will be asked to refer to the relevant sections of the code that will be modified. If you are building on top previous records provided by other reproducers to ACRE, and provide a brief justification of its validity and sensibility. Propose a reasonable alternative: identify line or section propose variation justify sensibility justify validity confirm that test is not redundant with other tests in the paper/robustness excercise. "]
]
